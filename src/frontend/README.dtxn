% HTable

A horizontally partitioned key/value mapping. This is for testing the
transaction system. It provides the mini-transaction API as a remote service.

Usage
=====

Run the htable simulation/test from the `trunk/` directory:

  java -ea -cp GPL_VCR/gpl_java/build/prod edu.mit.htable.HTableSim

This will produce output like the following:

  2 conflicts
  11 conflicts
  [...]
  21 conflicts
  13 conflicts

  Client 0:
  f = 117
  g = 117
  [...]
  y = 115
  x = 116

  Client 1:
  f = 200
  g = 200
  [...]
  y = 200
  x = 200

  Site 0:
  f = 200
  g = 200
  [...]
  h = 200
  i = 200

  Site 1:
  n = 200
  o = 200
  [...]
  y = 200
  x = 200

Overview
========

The data model in HTable is a key-value map from letters (a through z) to
integers (initialized to 0).  (Key-value pairs are heretofore called _items_.)

`HTableSim` starts a number of threads that behave as different nodes.  It
starts two clients and one server.  To run HTable in different processes, run
`HTableHost` and 2 `HTableIncrementer`s.  The HTable server nodes are all still
running in the same process (different threads represent different nodes), but
now the client is separated.  (Eventually the server nodes will reside in
distinct processes as well.)

In `HTableSim`/`HTableIncrementer`, the client connects to the sequencer and
issues transactions.  It randomly generates a bunch of transactions that
increment (update) some subset of the items.  The transactions are actually
paired up so that the first transaction is a read of all the involved items,
and the second is a write of all the involved items conditioned on the items'
values matching those read in the first transactions (that the items have not
changed is important for atomic increment semantics).

`HTableClient` is a much simpler client that simply issues a single
transaction.

`HTableListener` sits between the client and `HTableSite`.  It handles both the
requests from the client (forwards to the site) and the responses from the site
(forwards to the client).

An `HTableSite` represents a single partition of the database.  (Currently the
system has no replicas.)  The partitions are hard-coded (currently divided into
a-m and n-z); the `HTableSite`s don't actually need to know the partitions
(they just initialize all keys).  The actual database is just a Java `HashMap`.

`HTableSite` receives messages as:

1. transaction 1
2. decision for transaction 1
3. transaction 2
4. decision for transaction 2

Control Flow Outline/Notes
--------------------------

- `HTableHost.main()`
  - `NUM_HOSTS = NUM_SITES + 1`
  - create a `SimpleMsgRouter` for communication among the `NUM_HOSTS`
  - create `NUM_SITES` partitions
    - each partition creates a `SimpleDtxnConnection`
  - ...

Client Workload Details
-----------------------

Because the subsets of keys to operate on in each transaction are randomly
generated by a client that isn't aware of the partitioning, the transactions in
this test may frequently be multi-site (general) transactions.

Status
------

Unlike in the old C++ version of H-Store/dtxn/HTable, the current version
implements work units and dependencies.  This means that the old special-case
transaction that tries to reset the minimum record across all partitions can be
expressed natively.  (However, there are currently no test cases that exercise
this.)

The future may see that decisions for transaction $n$ piggy-back on the message
for transaction $n+1$.

<!--
vim:ft=mkd
-->
